{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUjZTQVbmI-4"
   },
   "source": [
    "## Classification with a Multi-layer Perceptron (MLP)\n",
    "\n",
    "In this problem set, we will *not* be implementing neural networks from scratch. Yesterday, you built a *perceptron* in Python. Multi-layer perceptrons (MLPs) are, as discussed in the lecture, several layers of these perceptrons stacked. Here, we will learn how to use one of the most common modules for building neural networks: Pytorch"
   ]
  },
<<<<<<< HEAD
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
=======
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Classification with a Multi-layer Perceptron (MLP)\n",
        "Author: V. Ashley Villar\n",
        "\n",
        "In this problem set, we will *not* be implementing neural networks from scratch. Yesterday, you built a *perceptron* in Python. Multi-layer perceptrons (MLPs) are, as discussed in the lecture, several layers of these perceptrons stacked. Here, we will learn how to use one of the most common modules for building neural networks: Pytorch"
      ],
      "metadata": {
        "id": "IUjZTQVbmI-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this module contains our dataset\n",
        "!pip install astronn\n",
        "#this is pytorch, which we will use to build our nn\n",
        "import torch\n",
        "#Standards for plotting, math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "#for our objective function\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "YZVxCOpuxNFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A few notes on Pytorch syntax \n",
        "(Many thanks to Vanessa Bohm!!)\n",
        "\n",
        "Pytorch datatype summary: The model expects a single precision input. You can change the type of a tensor with tensor_name.type(), where tensor_name is the name of your tensor and type is the dtype. For typecasting into single precision floating points, use float(). A numpy array is typecasted with array_name.astype(type). For single precision, the type should be np.float32.\n",
        "Before we analyze tensors we often want to convert them to numpy arrays with tensor_name.numpy()\n",
        "\n",
        "If pytorch has been tracking operations that resulted in the current tensor value, you need to detach the tensor from the graph (meaning you want to ignore things like its derivative) before you can transform it into a numpy array: tensor_name.detach(). Scalars can be detached with scalar.item()\n",
        "\n",
        "Pytorch allows you to easily use your CPU or GPU; however, we are not using this feature. If you tensor is currently on the GPU, you can bring it onto the CPU with tensor_name.cpu()"
      ],
      "metadata": {
        "id": "SHyzAlMby4gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1: Understanding the Data\n",
        "\n",
        "For this problem set, we will use the Galaxy10 dataset made available via the astroNN module. This dataset is made up of 17736 images of galaxies which have been labelled by hand. See this [link](https://astronn.readthedocs.io/en/latest/galaxy10.html) for more information. \n",
        "\n",
        "First we will visualize our data.\n",
        "\n",
        "**Problem 1a** Show one example of each class as an image.\n",
        "\n"
      ],
      "metadata": {
        "id": "xHiDLvC7SvGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from astroNN.datasets import load_galaxy10\n",
        "from astroNN.datasets.galaxy10 import galaxy10cls_lookup\n",
        "%matplotlib inline\n",
        "\n",
        "#helpful functions:\n",
        "#Load the images and labels as numbers\n",
        "images, labels_original = load_galaxy10()\n",
        "\n",
        "#convert numbers to a string\n",
        "galaxy10cls_lookup(labels_original[0])\n"
      ],
      "metadata": {
        "id": "vH8FF-l0TxXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2b** Make a histogram showing the fraction of each class\n",
        "\n",
        "Keep only the top two classes (i.e., the classes with the most galaxies)"
      ],
      "metadata": {
        "id": "dghsBXI3VXcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "images_top_two = ...\n",
        "labels_top_two = ..."
      ],
      "metadata": {
        "id": "PoClh9kFVV5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This next block of code converts the data to a format which is more compatible with our neural network."
      ],
      "metadata": {
        "id": "dcZOURvbWWZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code converts from integer labels to 'one-hot encodings'. What does that term mean?\n",
        "import torch.nn.functional as F\n",
        "torch.set_default_dtype(torch.float)\n",
        "labels_top_two_one_hot = F.one_hot(torch.tensor(labels_top_two - np.min(labels_top_two)).long(), num_classes=2)\n",
        "images_top_two = torch.tensor(images_top_two).float()\n",
        "labels_top_two_one_hot = labels_top_two_one_hot.float()\n",
        "\n",
        "\n",
        "# we're going to flatten the images for our MLP\n",
        "images_top_two_flat = ...\n",
        "\n",
        "#Normalize the flux of the images here\n",
        "images_top_two_flat_normed = ...\n"
      ],
      "metadata": {
        "id": "ZdTlL1sTmRbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Problem 2c** Split the data into a training and test set (66/33 split) using the train_test_split function from sklearn"
      ],
      "metadata": {
        "id": "WgkfExBpZA_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "CVCTXBKdrCVn"
      },
      "execution_count": null,
      "outputs": []
    },
>>>>>>> main
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.10.2-cp38-none-macosx_10_9_x86_64.whl (147.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 147.2 MB 205 kB/s eta 0:00:014\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from torch) (3.10.0.0)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.10.2\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YZVxCOpuxNFp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: astronn in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astronn) (3.2.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astronn) (21.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astronn) (4.62.2)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astronn) (0.24.2)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astronn) (3.4.2)\n",
      "Requirement already satisfied: astroquery in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astronn) (0.4.3)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astronn) (0.11.2)\n",
      "Requirement already satisfied: astropy in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astronn) (4.3.1)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astronn) (1.3.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astronn) (1.20.3)\n",
      "Requirement already satisfied: pyerfa>=1.7.3 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astropy->astronn) (2.0.0)\n",
      "Requirement already satisfied: requests>=2.4.3 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astroquery->astronn) (2.26.0)\n",
      "Requirement already satisfied: beautifulsoup4>=4.3.2 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astroquery->astronn) (4.9.3)\n",
      "Requirement already satisfied: html5lib>=0.999 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astroquery->astronn) (1.1)\n",
      "Requirement already satisfied: keyring>=4.0 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astroquery->astronn) (23.0.1)\n",
      "Requirement already satisfied: pyvo>=1.1 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astroquery->astronn) (1.1)\n",
      "Requirement already satisfied: six in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from astroquery->astronn) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from beautifulsoup4>=4.3.2->astroquery->astronn) (2.2.1)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from html5lib>=0.999->astroquery->astronn) (0.5.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from keyring>=4.0->astroquery->astronn) (3.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from importlib-metadata>=3.6->keyring>=4.0->astroquery->astronn) (3.5.0)\n",
      "Requirement already satisfied: mimeparse in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from pyvo>=1.1->astroquery->astronn) (0.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from requests>=2.4.3->astroquery->astronn) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from requests>=2.4.3->astroquery->astronn) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from requests>=2.4.3->astroquery->astronn) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from requests>=2.4.3->astroquery->astronn) (2.0.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from matplotlib->astronn) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from matplotlib->astronn) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from matplotlib->astronn) (8.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from matplotlib->astronn) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from matplotlib->astronn) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from pandas->astronn) (2021.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from scikit-learn->astronn) (1.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from scikit-learn->astronn) (2.2.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages (from scikit-learn->astronn) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "# this module contains our dataset\n",
    "!pip install astronn\n",
    "#this is pytorch, which we will use to build our nn\n",
    "import torch\n",
    "#Standards for plotting, math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#for our objective function\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHyzAlMby4gy"
   },
   "source": [
    "# A few notes on Pytorch syntax \n",
    "(Many thanks to Vanessa Bohm!!)\n",
    "\n",
    "Pytorch datatype summary: The model expects a single precision input. You can change the type of a tensor with tensor_name.type(), where tensor_name is the name of your tensor and type is the dtype. For typecasting into single precision floating points, use float(). A numpy array is typecasted with array_name.astype(type). For single precision, the type should be np.float32.\n",
    "Before we analyze tensors we often want to convert them to numpy arrays with tensor_name.numpy()\n",
    "\n",
    "If pytorch has been tracking operations that resulted in the current tensor value, you need to detach the tensor from the graph (meaning you want to ignore things like its derivative) before you can transform it into a numpy array: tensor_name.detach(). Scalars can be detached with scalar.item()\n",
    "\n",
    "Pytorch allows you to easily use your CPU or GPU; however, we are not using this feature. If you tensor is currently on the GPU, you can bring it onto the CPU with tensor_name.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHiDLvC7SvGR"
   },
   "source": [
    "# Problem 1: Understanding the Data\n",
    "\n",
    "For this problem set, we will use the Galaxy10 dataset made available via the astroNN module. This dataset is made up of 17736 images of galaxies which have been labelled by hand. See this [link](https://astronn.readthedocs.io/en/latest/galaxy10.html) for more information. \n",
    "\n",
    "First we will visualize our data.\n",
    "\n",
    "**Problem 1a** Show one example of each class as an image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vH8FF-l0TxXz"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/opt/anaconda3/envs/DSFP/lib/python3.8/site-packages/h5py/defs.cpython-38-darwin.so, 2): Symbol not found: _H5Pget_fapl_ros3\n  Referenced from: /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages/h5py/defs.cpython-38-darwin.so\n  Expected in: /opt/anaconda3/envs/DSFP/lib/libhdf5.103.dylib\n in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages/h5py/defs.cpython-38-darwin.so",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/j2/tlw62j517hb6ch7m280c_66h0000gn/T/ipykernel_8092/1047105765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mastroNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_galaxy10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mastroNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgalaxy10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgalaxy10cls_lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#helpful functions:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DSFP/lib/python3.8/site-packages/astroNN/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mastroNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapogee_distances\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_apogee_distances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mastroNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapogee_rc\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_apogee_rc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mastroNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgalaxy10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_data\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mload_galaxy10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mastroNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mH5Compiler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mastroNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mH5Loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DSFP/lib/python3.8/site-packages/astroNN/datasets/galaxy10.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DSFP/lib/python3.8/site-packages/h5py/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdf5_version_tuple\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhdf5_built_version_tuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/DSFP/lib/python3.8/site-packages/h5py/version.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mh5\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_h5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/h5.pyx\u001b[0m in \u001b[0;36minit h5py.h5\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/opt/anaconda3/envs/DSFP/lib/python3.8/site-packages/h5py/defs.cpython-38-darwin.so, 2): Symbol not found: _H5Pget_fapl_ros3\n  Referenced from: /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages/h5py/defs.cpython-38-darwin.so\n  Expected in: /opt/anaconda3/envs/DSFP/lib/libhdf5.103.dylib\n in /opt/anaconda3/envs/DSFP/lib/python3.8/site-packages/h5py/defs.cpython-38-darwin.so"
     ]
    }
   ],
   "source": [
    "from astroNN.datasets import load_galaxy10\n",
    "from astroNN.datasets.galaxy10 import galaxy10cls_lookup\n",
    "%matplotlib inline\n",
    "\n",
    "#helpful functions:\n",
    "#Load the images and labels as numbers\n",
    "images, labels_original = load_galaxy10()\n",
    "\n",
    "#convert numbers to a string\n",
    "galaxy10cls_lookup(labels_original[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dghsBXI3VXcY"
   },
   "source": [
    "**Problem 2b** Make a histogram showing the fraction of each class\n",
    "\n",
    "Keep only the top two classes (i.e., the classes with the most galaxies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoClh9kFVV5Y"
   },
   "outputs": [],
   "source": [
    "images_top_two = ...\n",
    "labels_top_two = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcZOURvbWWZt"
   },
   "source": [
    "This next block of code converts the data to a format which is more compatible with our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdTlL1sTmRbe"
   },
   "outputs": [],
   "source": [
    "# This code converts from integer labels to 'one-hot encodings'. What does that term mean?\n",
    "import torch.nn.functional as F\n",
    "torch.set_default_dtype(torch.float)\n",
    "labels_top_two_one_hot = F.one_hot(torch.tensor(labels_top_two - np.min(labels_top_two)).long(), num_classes=2)\n",
    "images_top_two = torch.tensor(images_top_two).float()\n",
    "labels_top_two_one_hot = labels_top_two_one_hot.float()\n",
    "\n",
    "\n",
    "# we're going to flatten the images for our MLP\n",
    "images_top_two_flat = ...\n",
    "\n",
    "#Normalize the flux of the images here\n",
    "images_top_two_flat_normed = ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgkfExBpZA_X"
   },
   "source": [
    "**Problem 2c** Split the data into a training and test set (66/33 split) using the train_test_split function from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVCTXBKdrCVn"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIAYeSPhZbQy"
   },
   "source": [
    "The next cell will outline how one can make a MLP with pytorch. \n",
    "\n",
    "**Problem 3a** Talk to a partner about how this code works, line by line. Add another hidden layer which is the same size as the first hidden layer. Choose an appropriate final nonlinear layer for this classification problem. Choose the appropriate number of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1VUC-2inrVD"
   },
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "      # this defines the model\n",
    "        def __init__(self, input_size, hidden_size):\n",
    "            super(MLP, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.hiddenlayer = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.outputlayer = torch.nn.Linear(self.hidden_size, HOW_MANY_OUTPUTS)\n",
    "            # some nonlinear options\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "            self.softmax = torch.nn.Softmax()\n",
    "            self.relu = torch.nn.ReLU()\n",
    "        def forward(self, x):\n",
    "            layer1 = self.hiddenlayer(x)\n",
    "            activation = self.sigmoid(layer1)\n",
    "            layer2 = self.outputlayer(activation)\n",
    "            output = self.NONLINEAR(layer2)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbDKkusZZ3D8"
   },
   "source": [
    "The next block of code will show how one can train the model for 100 epochs. Note that we use the *binary cross-entropy* as our objective function and *stochastic gradient descent* as our optimization method.\n",
    "\n",
    "**Problem 3b** Edit the code so that the function plots the loss for the training and test loss for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AT8vw_GnxL8F"
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "def train_model(training_data,training_labels, test_data,test_labels, model):\n",
    "  # define the optimization\n",
    "  criterion = torch.nn.BCELoss()\n",
    "  optimizer = torch.optim.SGD(model.parameters(), lr=0.1,momentum=0.9)\n",
    "  # Increase the number of epochs for your \"final\" run\n",
    "  for epoch in range(10):\n",
    "    # clear the gradient\n",
    "    optimizer.zero_grad()\n",
    "    # compute the model output\n",
    "    myoutput = model(training_data)\n",
    "    # calculate loss\n",
    "    loss = criterion(myoutput, training_labels)\n",
    "    # credit assignment\n",
    "    loss.backward()\n",
    "    # update model weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # ADD PLOT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E11xxIMWbEz2"
   },
   "source": [
    "The next block trains the code, assuming a hidden layer size of 100 neurons.\n",
    "\n",
    "**Problem 3c** Change the learning rate `lr` to minimize the cross entropy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HET50GP4bEJi"
   },
   "outputs": [],
   "source": [
    "model = MLP(np.shape(images_train[0])[0],100)\n",
    "train_model(images_train, labels_train, images_test, labels_test, model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suh_J0qWbOFV"
   },
   "source": [
    "Write a function called `evaluate_model` which takes the image data, labels and model as input, and the accuracy as output. you can use the `accuracy_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XM7S2spqSBFq"
   },
   "outputs": [],
   "source": [
    "# evaluate the model\n",
    "def evaluate_model(data,labels, model):\n",
    "  return(acc)\n",
    "# evaluate the model\n",
    "acc = evaluate_model(images_test,labels_test, model)\n",
    "print('Accuracy: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vQ_KFvIUj-sz"
   },
   "source": [
    "**Problem 3d** Make a confusion matrix for the test set using `confusiion_matrix` and 'ConfusionMatrixDisplay`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrnvQDHhkE91"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A59krpL_iPB-"
   },
   "source": [
    "**Challenge Problem** Add a third class to your classifier and begin accounting for uneven classes. There are several steps to this:\n",
    "\n",
    "1. Edit the neural network to output 3 classes\n",
    "2. Change the criterion to *Cross Entropy Loss* , such that the entropy of each class is weighted by the inverse fraction of each class size (e.g., if the galaxy class breakdowns are 1:2:3, the weights would be 6:3:2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4amk25yj9J9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DeeplearningBlank.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
